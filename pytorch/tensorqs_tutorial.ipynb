{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cheffjiu/pytorch-tutorials-zh/blob/main/tensorqs_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ke5-P5X4AAi"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djAD-3_T4AAj"
      },
      "source": [
        "[Learn the Basics](https://github.com/cheffjiu/pytorch-tutorials-zh/blob/main/intro.ipynb) \\|\\|\n",
        "[Quickstart](https://github.com/cheffjiu/pytorch-tutorials-zh/blob/main/quickstart_tutorial.ipynb) \\|\\| **Tensors** \\|\\| [Datasets &\n",
        "DataLoaders](https://github.com/cheffjiu/pytorch-tutorials-zh/blob/main/data_tutorial.ipynb) \\|\\|\n",
        "[Transforms](https://github.com/cheffjiu/pytorch-tutorials-zh/blob/main/transforms_tutorial.ipynb) \\|\\| [Build\n",
        "Model](https://github.com/cheffjiu/pytorch-tutorials-zh/blob/main/buildmodel_tutorial.ipynb) \\|\\|\n",
        "[Autograd](https://github.com/cheffjiu/pytorch-tutorials-zh/blob/main/autogradqs_tutorial.ipynb) \\|\\|\n",
        "[Optimization](https://github.com/cheffjiu/pytorch-tutorials-zh/blob/main/optimization_tutorial.ipynb) \\|\\| [Save & Load\n",
        "Model](https://github.com/cheffjiu/pytorch-tutorials-zh/blob/main/saveloadrun_tutorial.ipynb)\n",
        "\n",
        "Tensors（张量）\n",
        "=======\n",
        "\n",
        "张量是一种特殊的数据结构，与数组和矩阵非常相似。在PyTorch中，我们使用张量对模型的输入、输出以及模型参数进行编码。    \n",
        "\n",
        "张量与 [NumPy](https://numpy.org/) 的多维数组类似，不同的是张量可以在GPU或其他硬件加速器上运行。实际上，张量和NumPy数组常常可以共享相同的底层内存，无需复制数据。  (see\n",
        "`bridge-to-np-label`). 张量还针对自动求导进行了优化(我们会在 [Autograd](autogradqs_tutorial.html)这一节学习). 如果你熟悉`ndarray`，那么使用张量API会得心应手。如果不熟悉，那就继续学习吧！\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu5RqKCd4AAk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOmPAtXk4AAk"
      },
      "source": [
        "Initializing a Tensor（初始化张量）\n",
        "=====================\n",
        "\n",
        "张量可以通过多种方式初始化。请看以下示例：\n",
        "\n",
        "**直接从数据创建**\n",
        "\n",
        "张量可以直接从数据创建。数据类型会自动推断。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j07Oc734AAk"
      },
      "outputs": [],
      "source": [
        "data = [[1, 2],[3, 4]]\n",
        "x_data = torch.tensor(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sV8ArG84AAl"
      },
      "source": [
        "**从NumPy数组创建**\n",
        "\n",
        "张量可以从NumPy数组创建 (反之亦然 - 见\n",
        "`bridge-to-np-label`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc6wJuCS4AAl"
      },
      "outputs": [],
      "source": [
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZkDusau4AAl"
      },
      "source": [
        "**从另一个张量创建:**\n",
        "\n",
        "除非显式覆盖，否则新张量将保留源张量的属性（形状、数据类型）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-uvmAQV4AAl"
      },
      "outputs": [],
      "source": [
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LudAwEo44AAl"
      },
      "source": [
        "**使用随机值或常量值创建:**\n",
        "\n",
        "`shape`是张量维度的元组。在以下函数中，它决定了输出张量的维度。.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkCWlxGX4AAl"
      },
      "outputs": [],
      "source": [
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qfUF1lf4AAm"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGRWP8W-4AAm"
      },
      "source": [
        "Attributes of a Tensor（张量的属性）\n",
        "======================\n",
        "\n",
        "张量属性描述了它们的形状、数据类型以及存储它们的设备。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkZdEW8U4AAm"
      },
      "outputs": [],
      "source": [
        "tensor = torch.rand(3,4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NvKujAU4AAm"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN_veDff4AAm"
      },
      "source": [
        "Operations on Tensors（张量的操作）\n",
        "=====================\n",
        "\n",
        "1200 多种张量运算，包括算术运算、线性代数、矩阵操作（转置、索引、切片）、采样等，都有全面的描述\n",
        "[here](https://pytorch.org/docs/stable/torch.html).\n",
        "\n",
        "这些操作中的每一项都可以在 CPU 以及诸如 CUDA、MPS、MTIA 或 XPU 等[加速器](https://pytorch.org/docs/stable/torch.html#accelerators)上运行。如果你使用的是 Colab，可以通过依次点击“运行时”>“更改运行时类型”>“GPU”来分配一个加速器。.\n",
        "\n",
        "默认情况下，张量在CPU上创建。我们需要在检查加速器可用性之后，使用`.to`方法显式地将张量移动到加速器上。请记住，跨设备复制大型张量在时间和内存方面开销可能很大！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgAO4BSn4AAm"
      },
      "outputs": [],
      "source": [
        "# We move our tensor to the current accelerator if available\n",
        "if torch.accelerator.is_available():\n",
        "    tensor = tensor.to(torch.accelerator.current_accelerator())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opDj2bLO4AAm"
      },
      "source": [
        "尝试以下的一些操作。如果你熟悉NumPy API，那么使用Tensor API对你来说将轻而易举。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvuJ4DDz4AAm"
      },
      "source": [
        "**标准的类似NumPy的索引和切片操作:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5aJsp0C4AAm"
      },
      "outputs": [],
      "source": [
        "tensor = torch.ones(4, 4)\n",
        "print(f\"First row: {tensor[0]}\")\n",
        "print(f\"First column: {tensor[:, 0]}\")\n",
        "print(f\"Last column: {tensor[..., -1]}\")\n",
        "tensor[:,1] = 0\n",
        "print(tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBIORGAL4AAm"
      },
      "source": [
        "**连接张量** 你可以使用 `torch.cat` 沿着给定维度连接一系列张量。另请参阅 [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)，这是另一个与 `torch.cat` 略有不同的张量连接操作符。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzWMJU434AAm"
      },
      "outputs": [],
      "source": [
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQdzoNEn4AAm"
      },
      "source": [
        "**代数运算**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdcQMxoQ4AAm"
      },
      "outputs": [],
      "source": [
        "# This computes the matrix multiplication（矩阵乘法） between two tensors. y1, y2, y3 will have the same value\n",
        "# ``tensor.T`` returns the transpose（转置） of a tensor\n",
        "y1 = tensor @ tensor.T #@和matmul都是做矩阵乘法\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "\n",
        "y3 = torch.rand_like(y1)\n",
        "torch.matmul(tensor, tensor.T, out=y3)\n",
        "\n",
        "\n",
        "# This computes the element-wise product（对位元素相乘）. z1, z2, z3 will have the same value\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "\n",
        "z3 = torch.rand_like(tensor)\n",
        "torch.mul(tensor, tensor, out=z3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBtGMhr24AAm"
      },
      "source": [
        "**单元素张量** 如果你有一个单元素张量，例如通过将张量的所有值聚合为一个值，你可以使用 `item()` 将其转换为Python数值："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IkQ-l8j4AAn"
      },
      "outputs": [],
      "source": [
        "agg = tensor.sum()\n",
        "agg_item = agg.item()\n",
        "print(agg_item, type(agg_item))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4Vb15kl4AAn"
      },
      "source": [
        "**原地操作**：将结果存储到操作数中的操作称为原地操作。它们以 `_` 后缀表示。例如：`x.copy_(y)`、`x.t_()` 会改变 `x`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1F-36TZ4AAn"
      },
      "outputs": [],
      "source": [
        "print(f\"{tensor} \\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh0L2y9E4AAn"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>原地操作节省了一些内存，但在计算导数时可能会出现问题，因为会立即丢失历史信息。因此，不建议使用它们。</p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqurWgAX4AAn"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROpUJoZS4AAn"
      },
      "source": [
        "Bridge with NumPy {bridge-to-np-label}（与Numpy的联系）\n",
        "=================\n",
        "\n",
        "CPU 上的张量和 NumPy 数组可以共享它们的底层内存位置，改变其中一个会导致另一个也发生改变。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EI-jxok4AAn"
      },
      "source": [
        "张量转换为NumPy数组\n",
        "=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5z-d9HVf4AAn"
      },
      "outputs": [],
      "source": [
        "t = torch.ones(5)\n",
        "print(f\"t: {t}\")\n",
        "n = t.numpy()\n",
        "print(f\"n: {n}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gur5heeB4AAn"
      },
      "source": [
        "张量的变化会反映在NumPy数组中。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJiWy66a4AAn"
      },
      "outputs": [],
      "source": [
        "t.add_(1)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpKTLMKC4AAn"
      },
      "source": [
        "NumPy数组转张量\n",
        "=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QDBHSm24AAn"
      },
      "outputs": [],
      "source": [
        "n = np.ones(5)\n",
        "t = torch.from_numpy(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YVcOQJo4AAn"
      },
      "source": [
        "NumPy数组中的变化会反映在张量中。\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA-zvM2b4AAn"
      },
      "outputs": [],
      "source": [
        "np.add(n, 1, out=n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "补充：张量形状与维度的操作\n",
        "====================="
      ],
      "metadata": {
        "id": "J3oN1LxADAiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.view\n",
        "=====================\n",
        "`torch.view()`是 PyTorch中用于 改变张量（Tensor）的形状（shape） 的函数，类似于 NumPy 的 reshape()。它允许你在不改变张量数据的前提下，重新定义其维度（例如将一维张量转为二维矩阵）。\n",
        "\n",
        "核心特点：   \n",
        "\n",
        "保持数据不变：仅调整张量的形状，不修改实际内容。    \n",
        "\n",
        "必须连续存储：要求张量在内存中是连续的（可通过 is_contiguous() 检查）。    \n",
        "\n",
        "支持动态维度：使用 -1 自动推断某个维度的大小（例如 x.view(-1, 2)）。"
      ],
      "metadata": {
        "id": "RxFnMoOJDqqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#共享内容机制\n",
        "x = torch.tensor([[1, 2], [3, 4]])\n",
        "y = x.view(4)        # y 是 x 的视图\n",
        "y[0] = 100          # 修改 y 会影响 x\n",
        "print(x)            # [[100, 2], [3, 4]]"
      ],
      "metadata": {
        "id": "gSwSvFVoHNsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#动态推断-1\n",
        "x = torch.randn(2, 3, 4)  # shape = [2, 3, 4], 共 24 元素\n",
        "\n",
        "# 自动计算缺失维度\n",
        "y = x.view(2, -1)         # shape = [2, 12]（24/(2) = 12）\n",
        "z = x.view(-1, 6)         # shape = [4, 6]（24/6 = 4）"
      ],
      "metadata": {
        "id": "Wer1C_muHaGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 展平操作：将任何形状转为 1D\n",
        "x = torch.randn(2, 3, 4)\n",
        "flat = x.view(-1)         # shape = [24]\n",
        "\n",
        "# 增加/减少维度\n",
        "y = flat.view(2, 3, 4)    # 恢复原形状\n",
        "z = y.view(2, 1, 12)      # 插入新维度（常用于广播）"
      ],
      "metadata": {
        "id": "2FRhlJH0Hlfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "连续性（Contiguity）问题   \n",
        "触发场景：当张量经过转置（transpose()）、切片（[:, ::2]）等操作后，可能变为非连续存储。    \n",
        "\n",
        "解决方法：先调用 .contiguous() 再使用 view()"
      ],
      "metadata": {
        "id": "hH8BoM0KHybi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, 4).t()  # 转置后不连续\n",
        "y = x.contiguous().view(-1)"
      ],
      "metadata": {
        "id": "xAQo22hyH200"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "与    reshape() 的区别\n",
        "\n",
        "---\n",
        "\n",
        "操作\t是否复制数据\t是否要求连续\t适用场景\n",
        "\n",
        "\n",
        "view()\t否（视图）\t是\t已知张量连续的快速变形\n",
        "\n",
        "reshape()\t可能复制\t否\t不确定连续性时的安全变形"
      ],
      "metadata": {
        "id": "aYn8niImIGjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.reshpe\n",
        "=====================\n",
        "功能：调整张量（Tensor）形状，返回一个与原张量 共享数据内存（如果可能） 的新张量视图。     \n",
        "\n",
        "核心特点：\n",
        "支持任意形状调整（无论张量是否连续存储）。\n",
        "尽可能避免数据复制，但如果原张量不连续且无法重新组织内存，则自动复制数据以确保连续性。    \n",
        "\n",
        "与 view() 的关系：reshape() 的设计目标是更通用的形状调整工具，集成 view() 和 contiguous() 的逻辑"
      ],
      "metadata": {
        "id": "CVwExV1WJL_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#动态维度推断（-1 的使用）\n",
        "x = torch.randn(2, 3, 4)  # 总元素数 = 24\n",
        "y = x.reshape(2, -1)       # 形状 → [2, 12]\n",
        "z = x.reshape(-1, 6)       # 形状 → [4, 6]"
      ],
      "metadata": {
        "id": "RMIz65c5Jr67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#自动处理连续性问题\n",
        "\n",
        "# 非连续张量（如转置后的张量）\n",
        "x = torch.randn(3, 4).t()  # 形状 [4, 3], 非连续\n",
        "y = x.reshape(12)          # 自动调用 contiguous() 并复制数据\n",
        "print(y.is_contiguous())   # True"
      ],
      "metadata": {
        "id": "vLNu_iYKJ1kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "内存共享与复制的规则.  \n",
        "\n",
        "共享内存的条件：原张量连续存储且新形状合法 → 返回视图。    \n",
        "\n",
        "复制数据的条件：原张量非连续且无法直接映射到新形状 → 返回副本。"
      ],
      "metadata": {
        "id": "mbHsGhBzJ-RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 示例：共享内存\n",
        "x = torch.arange(6)          # 连续存储\n",
        "y = x.reshape(2, 3)          # 共享内存\n",
        "y[0, 0] = 100                # 修改 y 会影响 x\n",
        "print(x)                     # tensor([100, 1, 2, 3, 4, 5])\n",
        "\n",
        "# 示例：复制数据\n",
        "x = torch.arange(6).t()      # 非连续\n",
        "y = x.reshape(2, 3)          # 复制数据\n",
        "y[0, 0] = 100                # 修改 y 不影响 x\n",
        "print(x)                     # tensor([0, 1, 2, 3, 4, 5])"
      ],
      "metadata": {
        "id": "dioYX7JOKD_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.unsqueeze\n",
        "=====================\n",
        "`torch.unsqueeze()` 的作用是 在张量的指定维度位置插入一个大小为 1 的维度，用于扩展张量的形状（shape）。这是调整张量维度的重要工具,常用于适配神经网络层的输入格式或实现广播操作.     \n",
        "\n",
        "关键特性：\n",
        "维度索引支持负数：例如 dim=-1 表示最后一个维度之后插入。    \n",
        "\n",
        "不复制数据：返回的是原张量的视图（共享内存）。    \n",
        "\n",
        "维度灵活性：可多次插入不同位置的维度。\n",
        "\n"
      ],
      "metadata": {
        "id": "ecOTvVk5KRU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#在 1D 张量中插入维度\n",
        "x = torch.tensor([1, 2, 3])  # shape [3]\n",
        "\n",
        "# 在第 0 维度插入，变为 [1, 3]\n",
        "y = x.unsqueeze(0)\n",
        "print(y.shape)  # torch.Size([1, 3])\n",
        "\n",
        "# 在第 1 维度插入，变为 [3, 1]\n",
        "z = x.unsqueeze(1)\n",
        "print(z.shape)  # torch.Size([3, 1])"
      ],
      "metadata": {
        "id": "CLLpo7AsKvJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#在 2D 张量中插入维度\n",
        "x = torch.randn(2, 3)       # shape [2, 3]\n",
        "\n",
        "# 维度索引为负数\n",
        "y = x.unsqueeze(-1)         # shape [2, 3, 1]\n",
        "z = x.unsqueeze(-2)         # shape [2, 1, 3]"
      ],
      "metadata": {
        "id": "gW-8Sn-WTIBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#多次插入维度\n",
        "x = torch.randn(4)          # shape [4]\n",
        "\n",
        "y = x.unsqueeze(0).unsqueeze(-1)  # 先插入第0维 → [1,4]，再插入第-1维 → [1,4,1]\n",
        "print(y.shape)              # torch.Size([1, 4, 1])"
      ],
      "metadata": {
        "id": "DFlxggCATM0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.squeeze\n",
        "=====================\n",
        "torch.squeeze() 用于 移除张量（Tensor）中所有大小为 1 的维度，帮助简化张量形状。它是 torch.unsqueeze() 的逆操作，常用于神经网络输出后处理、数据压缩等场景。\n",
        "\n",
        "关键特性：\n",
        "自动移除空维度：删除所有 size=1 的维度（如 [1, 3, 1, 5] → [3, 5]）。       \n",
        "\n",
        "指定维度移除：通过 dim 参数可移除特定位置的维度（必须满足该维度大小为 1）。      \n",
        "\n",
        "内存共享：返回原张量的视图（不复制数据）。\n"
      ],
      "metadata": {
        "id": "WdAQi1w1TmLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#移除所有空维度\n",
        "x = torch.randn(1, 3, 1, 5)  # shape [1, 3, 1, 5]\n",
        "y = x.squeeze()               # 移除所有 size=1 的维度 → [3, 5]"
      ],
      "metadata": {
        "id": "rIBRMcPXUJ6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#指定维度移除\n",
        "\n",
        "x = torch.randn(3, 1, 4)     # shape [3, 1, 4]\n",
        "\n",
        "# 移除第 1 维（必须大小为 1）\n",
        "y = x.squeeze(dim=1)         # → [3, 4]\n",
        "\n",
        "# 若指定维度大小不为 1，会报错！\n",
        "try:\n",
        "    z = x.squeeze(dim=0)     # dim=0 的大小是 3（非 1）\n",
        "except Exception as e:\n",
        "    print(\"错误:\", e)        # \"squeeze() 指定的维度大小不为 1\""
      ],
      "metadata": {
        "id": "c8ThAn5IUQw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#多次调用\n",
        "x = torch.randn(1, 1, 5)     # shape [1, 1, 5]\n",
        "\n",
        "# 分步骤移除\n",
        "y = x.squeeze(dim=0)         # → [1, 5]\n",
        "z = y.squeeze()              # → [5]"
      ],
      "metadata": {
        "id": "iWYZ7WE8UcF4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}