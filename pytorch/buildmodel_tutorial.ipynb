{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cheffjiu/pytorch-tutorials-zh/blob/main/buildmodel_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gumCdwhkxu_a"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijp18oD3xu_b"
      },
      "source": [
        "[Learn the Basics](https://github.com/cheffjiu/pytorch-tutorials-zh/blob/main/intro.ipynb) \\|\\|\n",
        "[Quickstart](https://github.com/cheffjiu/pytorch-tutorials-zh/blob/main/quickstart_tutorial.ipynb) \\|\\|\n",
        "[Tensors](https://github.com/cheffjiu/pytorch-tutorials-zh/blob/main/tensorqs_tutorial.ipynb) \\|\\| [Datasets &\n",
        "DataLoaders](https://github.com/cheffjiu/pytorch-tutorials-zh/blob/main/data_tutorial.ipynb) \\|\\|\n",
        "[Transforms](https://github.com/cheffjiu/pytorch-tutorials-zh/blob/main/transforms_tutorial.ipynb) \\|\\| **Build Model** \\|\\|\n",
        "[Autograd](https://github.com/cheffjiu/pytorch-tutorials-zh/blob/main/autogradqs_tutorial.ipynb) \\|\\|\n",
        "[Optimization](https://github.com/cheffjiu/pytorch-tutorials-zh/blob/main/optimization_tutorial.ipynb) \\|\\| [Save & Load\n",
        "Model](https://github.com/cheffjiu/pytorch-tutorials-zh/blob/main/saveloadrun_tutorial.ipynb)\n",
        "\n",
        "Build the Neural Network(构建神经网络)\n",
        "========================\n",
        "\n",
        "神经网络由对数据执行操作的层/模块组成。 [torch.nn](https://pytorch.org/docs/stable/nn.html)命名空间提供了构建自己的神经网络所需的所有基础组件。 PyTorch中的每个模块都是[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)的子类。\n",
        "神经网络本身就是一个模块，它由其他模块（层）组成。这种嵌套结构便于轻松构建和管理复杂的架构。\n",
        "\n",
        "在接下来的章节中，我们将构建一个神经网络，对FashionMNIST数据集中的图像进行分类。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSwNsQiexu_c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4LR2hY5xu_d"
      },
      "source": [
        "Get Device for Training（获取训练设备）\n",
        "=======================\n",
        "\n",
        "我们希望能够在诸如CUDA、MPS、MTIA或XPU等[加速器](https://pytorch.org/docs/stable/torch.html#accelerators)上训练模型。如果当前的加速器可用，我们将使用它。否则，我们就使用CPU。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siueJAr4xu_d"
      },
      "outputs": [],
      "source": [
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsUWYFZqxu_d"
      },
      "source": [
        "Define the Class（定义模型类）\n",
        "================\n",
        "\n",
        "我们通过继承`nn.Module`来定义神经网络，并在`__init__`中初始化神经网络层。每个`nn.Module`子类都在`forward`方法中实现对输入数据的操作。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ6Pan1Rxu_d"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veBdC85Axu_d"
      },
      "source": [
        "我们创建一个`NeuralNetwork`实例，将其移动到`device`上，并打印其结构。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldrf44w-xu_d"
      },
      "outputs": [],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdblIYGUxu_e"
      },
      "source": [
        "要使用该模型，我们将输入数据传递给它。这会执行模型的 `forward` 方法， 同时也有[\n",
        "operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866).\n",
        "不要直接调用`model.forward()`！\n",
        "\n",
        "对输入调用模型会返回一个二维张量，其中维度dim=0对应每个类别的10个原始预测值的每个输出，维度dim=1对应每个输出的各个值。我们通过将其传入`nn.Softmax`模块的实例来获得预测概率。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxXFPXeTxu_e"
      },
      "outputs": [],
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydCUX89kxu_e"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCy1kgXdxu_e"
      },
      "source": [
        "Model Layers(模型层)\n",
        "============\n",
        "\n",
        "让我们剖析FashionMNIST模型中的各层。为了说明这一点，我们将取一个包含3张28x28尺寸图像的小批量样本，看看当它通过网络时会发生什么。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL5touhgxu_e"
      },
      "outputs": [],
      "source": [
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBbcGcOWxu_e"
      },
      "source": [
        "nn.Flatten\n",
        "==========\n",
        "\n",
        "我们初始化\n",
        "[nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)\n",
        "层，将每个2D的28x28图像转换为一个包含784个像素值的连续数组（小批量维度（在维度0处）保持不变）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dwlyUoRxu_e"
      },
      "outputs": [],
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSmvxzMQxu_e"
      },
      "source": [
        "nn.Linear\n",
        "=========\n",
        "\n",
        "[线性层](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)是一个模块，它使用存储的weights和bias对输入进行线性变换。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ok5vfBJxu_e"
      },
      "outputs": [],
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKaoQe47xu_e"
      },
      "source": [
        "nn.ReLU\n",
        "=======\n",
        "\n",
        "非线性激活函数构建了模型输入与输出之间的复杂映射关系。它们在线性变换之后应用，以引入“非线性”，帮助神经网络学习各种各样的现象。\n",
        "\n",
        "在这个模型中，我们在线性层之间使用[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)，但也有其他激活函数可以在你的模型中引入非线性。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyhLj9Pexu_e"
      },
      "outputs": [],
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlJs_xCFxu_e"
      },
      "source": [
        "nn.Sequential\n",
        "=============\n",
        "\n",
        "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) 是一个有序的模块容器。数据按照定义的相同顺序依次通过所有模块。你可以使用顺序容器来快速搭建一个类似 `seq_modules` 的网络。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6XV2zlLxu_e"
      },
      "outputs": [],
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAWPliF0xu_e"
      },
      "source": [
        "nn.Softmax\n",
        "==========\n",
        "\n",
        "神经网络的最后一个线性层返回 [logits]\n",
        "- raw values 在 \\[-∞, +∞\\] 中- 这些值会被传递给[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)模块。`logits`会被缩放到[0, 1]范围内的值，这些值代表模型对每个类别的预测概率。`dim`参数表示值必须沿其求和为1的维度。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JMOpfvFxu_f"
      },
      "outputs": [],
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsxOgwyrxu_f"
      },
      "source": [
        "Model Parameters(模型参数)\n",
        "================\n",
        "\n",
        "神经网络内部的许多层都是**参数化的**，即具有在训练过程中进行优化的相关权重和偏差。继承 `nn.Module` 会自动跟踪模型对象内部定义的所有字段，并通过模型的 `parameters()` 或 `named_parameters()` 方法访问所有参数。\n",
        "\n",
        "在这个例子中，我们遍历每个参数，并打印其大小以及其值的预览。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5ejrDYRxu_f"
      },
      "outputs": [],
      "source": [
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEzbBNPgxu_f"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx6XHsZJxu_f"
      },
      "source": [
        "Further Reading\n",
        "===============\n",
        "\n",
        "-   [torch.nn API](https://pytorch.org/docs/stable/nn.html)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}